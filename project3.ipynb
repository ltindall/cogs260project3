{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train = []\n",
    "train_labels = []\n",
    "train_both = []\n",
    "file = open('iris/iris_train.data', 'r') \n",
    "for line in file: \n",
    "    l = line.rstrip('\\n').split(',')\n",
    "    \n",
    "    nums = [float(x) for x in l[:4]]\n",
    "    train_labels.append(l[-1])\n",
    "    nums.append(l[-1])\n",
    "    train_both.append(nums)\n",
    "    train.append(nums[0:-1])\n",
    "    \n",
    "    \n",
    "test = []\n",
    "test_labels = []\n",
    "test_both = []\n",
    "file = open('iris/iris_test.data', 'r') \n",
    "for line in file: \n",
    "    l = line.rstrip('\\n').split(',')\n",
    "    \n",
    "    nums = [float(x) for x in l[:4]]\n",
    "    test_labels.append(l[-1])\n",
    "    nums.append(l[-1])\n",
    "    test_both.append(nums)\n",
    "    test.append(nums[0:-1])\n",
    "\n",
    "test = np.array(test)\n",
    "test_labels = np.array(test_labels)\n",
    "test_both = np.array(test_both)\n",
    "#print train_both\n",
    "#print train\n",
    "train = np.array(train)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "train, train_labels = shuffle(train, train_labels, random_state=0)\n",
    "\n",
    "unique_labels = np.unique(train_labels)\n",
    "\n",
    "\n",
    "\n",
    "sepal_length_setosa = [x[0] for x in train_both if x[-1] == unique_labels[0]]\n",
    "sepal_width_setosa = [x[1] for x in train_both if x[-1] == unique_labels[0]]\n",
    "petal_length_setosa = [x[2] for x in train_both if x[-1] == unique_labels[0]]\n",
    "petal_width_setosa = [x[3] for x in train_both if x[-1] == unique_labels[0]]\n",
    "\n",
    "sepal_length_versicolor = [x[0] for x in train_both if x[-1] == unique_labels[1]]\n",
    "sepal_width_versicolor = [x[1] for x in train_both if x[-1] == unique_labels[1]]\n",
    "petal_length_versicolor = [x[2] for x in train_both if x[-1] == unique_labels[1]]\n",
    "petal_width_versicolor = [x[3] for x in train_both if x[-1] == unique_labels[1]]\n",
    "\n",
    "\n",
    "plt.plot(sepal_length_setosa, sepal_width_setosa, 'g^',label='setosa')\n",
    "plt.plot(sepal_length_versicolor, sepal_width_versicolor, 'bo',label='versicolor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sepal_length_setosa, petal_length_setosa, 'g^',label='setosa')\n",
    "plt.plot(sepal_length_versicolor, petal_length_versicolor, 'bo',label='versicolor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal length')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sepal_length_setosa, petal_width_setosa, 'g^',label='setosa')\n",
    "plt.plot(sepal_length_versicolor, petal_width_versicolor, 'bo',label='versicolor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal width')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sepal_width_setosa, petal_length_setosa, 'g^',label='setosa')\n",
    "plt.plot(sepal_width_versicolor, petal_length_versicolor, 'bo',label='versicolor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('sepal width')\n",
    "plt.ylabel('petal length')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sepal_width_setosa, petal_width_setosa, 'g^',label='setosa')\n",
    "plt.plot(sepal_width_versicolor, petal_width_versicolor, 'bo',label='versicolor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('sepal width')\n",
    "plt.ylabel('petal width')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(petal_length_setosa, petal_width_setosa, 'g^',label='setosa')\n",
    "plt.plot(petal_length_versicolor, petal_width_versicolor, 'bo',label='versicolor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('petal width')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def perceptronAccuracy(data, labels, weights):\n",
    "\n",
    "    missed = 0\n",
    "\n",
    "    testWithBias = np.append(np.ones((len(data),1)),data,axis=1)\n",
    "\n",
    "    for sample,label in zip(testWithBias,labels): \n",
    "        predicted = int(np.dot(sample,weights) >= 0)\n",
    "        \n",
    "    \n",
    "        if predicted != label: \n",
    "            missed = missed + 1\n",
    "\n",
    "    correct = len(labels) - missed \n",
    "    accuracy = 1.0 * correct / len(labels)\n",
    "    print \"Perceptron classifiction accuracy = \",accuracy * 100,\"%\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trainPerceptron(learn_rate, data, labels): \n",
    "    weights = np.array([0.0 for x in range(data.shape[1]+1)])\n",
    "    errors = 1\n",
    "    iterations = 0\n",
    "    \n",
    "    trainWithBias = np.append(np.ones((len(data),1)),data,axis=1)\n",
    "    \n",
    "    while errors: \n",
    "        errors = 0\n",
    "        iterations = iterations + 1 \n",
    "\n",
    "        for sample,label in zip(trainWithBias,labels): \n",
    "\n",
    "            predicted = int(np.dot(sample,weights) >= 0)\n",
    "\n",
    "            error = label - predicted\n",
    "\n",
    "            if error != 0: \n",
    "                errors = errors + 1\n",
    "\n",
    "            weights = weights + (learn_rate * error * sample)\n",
    "            \n",
    "    print \"Number of iterations till convergence = \",iterations \n",
    "    return weights \n",
    "    \n",
    "    \n",
    "    \n",
    "learning_rate = 0.1\n",
    "\n",
    "train_labels_perceptron = np.array([0 if x == unique_labels[0] else 1 for x in train_labels])\n",
    "test_labels_perceptron = np.array([0 if x == unique_labels[0] else 1 for x in test_labels])\n",
    "\n",
    "w = trainPerceptron(learning_rate, train, train_labels_perceptron)        \n",
    "perceptronAccuracy(test, test_labels_perceptron, w)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Z-score the training and test data \n",
    "\n",
    "trainZScored = np.copy(train)\n",
    "testZScored = np.copy(test)\n",
    "\n",
    "for i in range(train.shape[1]): \n",
    "    m = np.mean(train[:,i])\n",
    "    s = np.std(train[:,i])\n",
    "    \n",
    "    for x in range(len(trainZScored[:,i])):\n",
    "        \n",
    "        trainZScored[x,i] = (trainZScored[x,i] - m) / s\n",
    "\n",
    "    for x in range(len(testZScored[:,i])): \n",
    "        testZScored[x,i] = (testZScored[x,i] - m) / s\n",
    "        \n",
    "        \n",
    "        \n",
    "learning_rate = 0.1\n",
    "\n",
    "wZScored = trainPerceptron(learning_rate, trainZScored, train_labels_perceptron)        \n",
    "perceptronAccuracy(testZScored, test_labels_perceptron, wZScored)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "training_samples = []\n",
    "training_labels = []\n",
    "for input,label in training_data: \n",
    "    training_samples.append(input)\n",
    "    training_labels.append(label)\n",
    "    \n",
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "for input,label in test_data: \n",
    "    test_samples.append(input)\n",
    "    test_labels.append(label)\n",
    "    \n",
    "training_samples = np.array(training_samples).reshape((50000,1,784))\n",
    "training_labels = np.array(training_labels).reshape((50000,10))\n",
    "test_samples = np.array(test_samples).reshape((10000,1,784))\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeedForwardNeuralNetwork: \n",
    "    def __init__(self, size_input, size_hidden, size_output):\n",
    "        \n",
    "        self.input_size = size_input\n",
    "        self.hidden_size = size_hidden\n",
    "        self.output_size = size_output\n",
    "        \n",
    "        self.hidden_weight = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.output_weight = np.random.randn(self.hidden_size, self.output_size)\n",
    "        \n",
    "        self.hidden_bias = np.random.randn(1,self.hidden_size) \n",
    "        self.output_bias = np.random.randn(1,self.output_size)\n",
    "        \n",
    "        self.output_velocity = np.zeros(self.output_weight.shape)\n",
    "        self.hidden_velocity = np.zeros(self.hidden_weight.shape)\n",
    "        \n",
    "        self.output_bias_velocity = np.zeros(self.output_bias.shape)\n",
    "        self.hidden_bias_velocity = np.zeros(self.hidden_bias.shape)\n",
    "        \n",
    "        self.training_x = [0]\n",
    "        self.training_y = [1]\n",
    "        \n",
    "        self.test_x = [0]\n",
    "        self.test_y = [1]\n",
    "        \n",
    "    def feed_forward(self, input): \n",
    "        \n",
    "        self.current_inputs = input\n",
    "        \n",
    "        # calculate hidden output -> output = (Inputs * weights) + biases\n",
    "        # apply activation function -> hidden activation = sigmoid(output)\n",
    "        \n",
    "        # (1,5)\n",
    "        self.hidden_activations = self.sigmoid(np.dot(input,self.hidden_weight) + self.hidden_bias)\n",
    "        \n",
    "        \n",
    "        # calculate output output -> output = (hidden activations * weights) + biases\n",
    "        # apply activation function -> output activation = sigmoid(output)\n",
    "        self.output_activations = self.sigmoid(np.dot(self.hidden_activations,self.output_weight) + self.output_bias)\n",
    "        \n",
    "        return self.output_activations \n",
    "    \n",
    "    def feed_forward_regularization(self, input, p): \n",
    "        \n",
    "        self.current_inputs = input\n",
    "        \n",
    "        # calculate hidden output -> output = (Inputs * weights) + biases\n",
    "        # apply activation function -> hidden activation = sigmoid(output)\n",
    "        \n",
    "        # (1,5)\n",
    "        self.hidden_activations = self.sigmoid(np.dot(input,self.hidden_weight) + self.hidden_bias)\n",
    "        \n",
    "        \n",
    "        drop_hidden = (np.random.rand(*self.hidden_activations.shape) < p ) \n",
    "        \n",
    "        self.hidden_activations = self.hidden_activations * drop_hidden\n",
    "        \n",
    "        \n",
    "        # calculate output output -> output = (hidden activations * weights) + biases\n",
    "        # apply activation function -> output activation = sigmoid(output)\n",
    "        self.output_activations = self.sigmoid(np.dot(self.hidden_activations,self.output_weight) + self.output_bias)\n",
    "        \n",
    "        return self.output_activations \n",
    "        \n",
    "    # alpha = Learning rate \n",
    "    def backpropagation_mom(self, output, alpha, mu):\n",
    "\n",
    "        target = output\n",
    "        \n",
    "        #(1,10)\n",
    "        output_delta = (self.output_activations - target) * self.derivative_sigmoid(self.output_activations)\n",
    "        \n",
    "        #(1,5)\n",
    "        hidden_delta = (np.dot(output_delta,self.output_weight.T)) * self.derivative_sigmoid(self.hidden_activations)\n",
    "        \n",
    "        #(1,784) \n",
    "        hidden_gradient = np.dot(self.current_inputs.T,hidden_delta)\n",
    "        \n",
    "        # (1,5)(1,10)\n",
    "        output_gradient = np.dot(self.hidden_activations.T, output_delta)\n",
    "        \n",
    "        output_bias_gradient = output_delta\n",
    "        hidden_bias_gradient = hidden_delta\n",
    "        \n",
    "        self.output_velocity = self.output_velocity * mu + (-1 * alpha * output_gradient)\n",
    "        self.output_weight = self.output_weight + self.output_velocity\n",
    "        \n",
    "        self.hidden_velocity = self.hidden_velocity * mu + (-1 * alpha * hidden_gradient)\n",
    "        self.hidden_weight = self.hidden_weight + self.hidden_velocity\n",
    "        \n",
    "        #self.output_weight = self.output_weight + (-1 * alpha * output_gradient)\n",
    "        #self.hidden_weight = self.hidden_weight + (-1 * alpha * hidden_gradient)\n",
    "        \n",
    "        self.output_bias_velocity = self.output_bias_velocity * mu + (-1 * alpha * output_bias_gradient)\n",
    "        self.output_bias = self.output_bias + self.output_bias_velocity\n",
    "        \n",
    "        self.hidden_bias_velocity = self.hidden_bias_velocity * mu + (-1 * alpha * hidden_bias_gradient)\n",
    "        self.hidden_bias = self.hidden_bias + self.hidden_bias_velocity\n",
    "        \n",
    "        #self.output_bias = self.output_bias + (-1 * alpha * output_bias_gradient)\n",
    "        #self.hidden_bias = self.hidden_bias + (-1 * alpha * hidden_bias_gradient)\n",
    "        \n",
    "    # alpha = Learning rate \n",
    "    def backpropagation(self, output, alpha):\n",
    "        target = output\n",
    "        \n",
    "        #(1,10)\n",
    "        output_delta = (self.output_activations - target) * self.derivative_sigmoid(self.output_activations)\n",
    "        \n",
    "        #(1,5)\n",
    "        hidden_delta = (np.dot(output_delta,self.output_weight.T)) * self.derivative_sigmoid(self.hidden_activations)\n",
    "        \n",
    "        #(1,784) \n",
    "        hidden_gradient = np.dot(self.current_inputs.T,hidden_delta)\n",
    "        \n",
    "        # (1,5)(1,10)\n",
    "        output_gradient = np.dot(self.hidden_activations.T, output_delta)\n",
    "        \n",
    "        output_bias_gradient = output_delta\n",
    "        hidden_bias_gradient = hidden_delta\n",
    "        \n",
    "        self.output_weight = self.output_weight + (-1 * alpha * output_gradient)\n",
    "        self.hidden_weight = self.hidden_weight + (-1 * alpha * hidden_gradient)\n",
    "        \n",
    "        self.output_bias = self.output_bias + (-1 * alpha * output_bias_gradient)\n",
    "        self.hidden_bias = self.hidden_bias + (-1 * alpha * hidden_bias_gradient)\n",
    "    \n",
    "    def predict(self,input, withRegMom, p): \n",
    "        \n",
    "        predictions = []\n",
    "        for i in input: \n",
    "            if withRegMom: \n",
    "                predictions.append(self.feed_forward_regularization(i,p))\n",
    "            else: \n",
    "                predictions.append(self.feed_forward(i))\n",
    "        return np.array(predictions)\n",
    "            \n",
    "    def train(self, training_samples, training_labels, test_samples, test_labels, alpha, epochs, withRegMom, p, mu):\n",
    "        \n",
    "        iterations = 0\n",
    "        for i in range(epochs): \n",
    "            \n",
    "            for input,label in zip(training_samples, training_labels):\n",
    "                if withRegMom: \n",
    "                    self.feed_forward_regularization(input,p)\n",
    "                    self.backpropagation_mom(label,alpha,mu)\n",
    "                else: \n",
    "                    self.feed_forward(input)\n",
    "                    self.backpropagation(label, alpha)\n",
    "                iterations = iterations + 1\n",
    "            \n",
    "        \n",
    "            #predictions = np.argmax(np.asarray(self.predict(training_samples)), axis=1)\n",
    "           \n",
    "            targets = np.argmax(np.asarray(training_labels), axis=1)\n",
    "            predictions = self.predict(training_samples, withRegMom, p)\n",
    "            \n",
    "            \n",
    "            errors = 0\n",
    "            for target,predicted in zip(targets,predictions): \n",
    "                predicted = np.argmax(np.asarray(predicted), axis=1)\n",
    "                if target != predicted: \n",
    "                    errors = errors + 1\n",
    "            error_rate = 1.0 * errors / len(training_labels)\n",
    "            \n",
    "            print iterations,' iterations, training error rate = ',error_rate\n",
    "            \n",
    "            self.training_x.append(self.training_x[-1]+1)\n",
    "            self.training_y.append(error_rate)\n",
    "            \n",
    "            \n",
    "            \n",
    "            targets = test_labels\n",
    "            predictions = self.predict(test_samples, withRegMom, p)\n",
    "            \n",
    "            \n",
    "            errors = 0\n",
    "            for target,predicted in zip(targets,predictions): \n",
    "                predicted = np.argmax(np.asarray(predicted), axis=1)\n",
    "                if target != predicted: \n",
    "                    errors = errors + 1\n",
    "            error_rate = 1.0 * errors / len(test_labels)\n",
    "            \n",
    "            self.test_x.append(self.test_x[-1]+1)\n",
    "            self.test_y.append(error_rate)\n",
    "            \n",
    "            print iterations,' iterations, test error rate = ',error_rate\n",
    "            \n",
    "    \n",
    "                \n",
    "    def sigmoid(self, t):\n",
    "        \n",
    "        return 1.0/(1.0+np.exp(-t))\n",
    "\n",
    "    def derivative_sigmoid(self, t): \n",
    "        return t * (1.0 - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ffnet = FeedForwardNeuralNetwork(784, 20, 10)\n",
    "#ffnet.train(training_samples,training_labels,test_samples,test_labels,0.1,100, False, 0,0)\n",
    "ffnet.train(training_samples,training_labels,test_samples,test_labels,0.1,100, True, 0.95,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)),plt.subplot(121),\n",
    "plt.plot(ffnet.training_x[1:],ffnet.training_y[1:],'b', label='training error')\n",
    "plt.plot(ffnet.test_x[1:],ffnet.test_y[1:],'r', label='test error')\n",
    "plt.title('Training and test error rate on network with 2 hidden layers')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('error rate')\n",
    "plt.legend(loc='upper center', shadow=True)\n",
    "plt.ylim(min(min(ffnet.training_y[1:]),min(ffnet.test_y[1:])),max(max(ffnet.training_y[1:]),max(ffnet.test_y[1:])))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork2Hidden: \n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2, size_output):\n",
    "        \n",
    "        self.input_size = size_input\n",
    "        self.hidden1_size = size_hidden1\n",
    "        self.hidden2_size = size_hidden2\n",
    "        self.output_size = size_output\n",
    "        \n",
    "        self.hidden1_weight = np.random.randn(self.input_size, self.hidden1_size)\n",
    "        self.hidden2_weight = np.random.randn(self.hidden1_size, self.hidden2_size)\n",
    "        self.output_weight = np.random.randn(self.hidden2_size, self.output_size)\n",
    "        \n",
    "        self.hidden1_bias = np.random.randn(1,self.hidden1_size) \n",
    "        self.hidden2_bias = np.random.randn(1,self.hidden2_size) \n",
    "        self.output_bias = np.random.randn(1,self.output_size)\n",
    "\n",
    "        self.training_x = [0]\n",
    "        self.training_y = [1]\n",
    "        \n",
    "        self.test_x = [0]\n",
    "        self.test_y = [1]\n",
    "\n",
    "        \n",
    "       \n",
    "    def feed_forward(self, input): \n",
    "        \n",
    "        self.current_inputs = input\n",
    "        \n",
    "        # calculate hidden output -> output = (Inputs * weights) + biases\n",
    "        # apply activation function -> hidden activation = sigmoid(output)\n",
    "        \n",
    "        # (1,5)\n",
    "        self.hidden1_activations = self.sigmoid(np.dot(input,self.hidden1_weight) + self.hidden1_bias)\n",
    "        \n",
    "        \n",
    "        self.hidden2_activations = self.sigmoid(np.dot(self.hidden1_activations,self.hidden2_weight) + self.hidden2_bias)\n",
    "        \n",
    "        \n",
    "        # calculate output output -> output = (hidden activations * weights) + biases\n",
    "        # apply activation function -> output activation = sigmoid(output)\n",
    "        self.output_activations = self.sigmoid(np.dot(self.hidden2_activations,self.output_weight) + self.output_bias)\n",
    "        \n",
    "        return self.output_activations \n",
    "    \n",
    "   \n",
    "        \n",
    "    # alpha = Learning rate \n",
    "    def backpropagation(self, output, alpha):\n",
    "        target = output\n",
    "        \n",
    "        #(1,10)\n",
    "        output_delta = (self.output_activations - target) * self.derivative_sigmoid(self.output_activations)\n",
    "        \n",
    "        #(1,5)\n",
    "        hidden2_delta = (np.dot(output_delta,self.output_weight.T)) * self.derivative_sigmoid(self.hidden2_activations)\n",
    "        \n",
    "        hidden1_delta = (np.dot(hidden2_delta,self.hidden2_weight.T)) * self.derivative_sigmoid(self.hidden1_activations)\n",
    "        \n",
    "        # (1,5)(1,10)\n",
    "        output_gradient = np.dot(self.hidden2_activations.T, output_delta)\n",
    "        \n",
    "        #(1,784) \n",
    "        hidden2_gradient = np.dot(self.hidden1_activations.T,hidden2_delta)\n",
    "        \n",
    "        hidden1_gradient = np.dot(self.current_inputs.T,hidden1_delta)\n",
    "        \n",
    "       \n",
    "        \n",
    "        output_bias_gradient = output_delta\n",
    "        hidden2_bias_gradient = hidden2_delta\n",
    "        hidden1_bias_gradient = hidden1_delta\n",
    "        \n",
    "        self.output_weight = self.output_weight + (-1 * alpha * output_gradient)\n",
    "        self.hidden2_weight = self.hidden2_weight + (-1 * alpha * hidden2_gradient)\n",
    "        self.hidden1_weight = self.hidden1_weight + (-1 * alpha * hidden1_gradient)\n",
    "        \n",
    "        self.output_bias = self.output_bias + (-1 * alpha * output_bias_gradient)\n",
    "        self.hidden2_bias = self.hidden2_bias + (-1 * alpha * hidden2_bias_gradient)\n",
    "        self.hidden1_bias = self.hidden1_bias + (-1 * alpha * hidden1_bias_gradient)\n",
    "    \n",
    "    def predict(self,input): \n",
    "        \n",
    "        predictions = []\n",
    "        for i in input: \n",
    "            predictions.append(self.feed_forward(i))\n",
    "        return np.array(predictions)\n",
    "            \n",
    "    def train(self, training_samples, training_labels, test_samples, test_labels, alpha, epochs):\n",
    "        \n",
    "        iterations = 0\n",
    "        for i in range(epochs): \n",
    "            \n",
    "            for input,label in zip(training_samples, training_labels):\n",
    "                self.feed_forward(input)\n",
    "                self.backpropagation(label, alpha)\n",
    "                iterations = iterations + 1\n",
    "            \n",
    "        \n",
    "            #predictions = np.argmax(np.asarray(self.predict(training_samples)), axis=1)\n",
    "           \n",
    "            targets = np.argmax(np.asarray(training_labels), axis=1)\n",
    "            predictions = self.predict(training_samples)\n",
    "            \n",
    "            \n",
    "            errors = 0\n",
    "            for target,predicted in zip(targets,predictions): \n",
    "                predicted = np.argmax(np.asarray(predicted), axis=1)\n",
    "                if target != predicted: \n",
    "                    errors = errors + 1\n",
    "            error_rate = 1.0 * errors / len(training_labels)\n",
    "            self.training_x.append(self.training_x[-1]+1)\n",
    "            self.training_y.append(error_rate)\n",
    "            \n",
    "            \n",
    "            print iterations,' iterations, training error rate = ',error_rate\n",
    "            \n",
    "            \n",
    "            targets = test_labels\n",
    "            predictions = self.predict(test_samples)\n",
    "            \n",
    "            \n",
    "            errors = 0\n",
    "            for target,predicted in zip(targets,predictions): \n",
    "                predicted = np.argmax(np.asarray(predicted), axis=1)\n",
    "                if target != predicted: \n",
    "                    errors = errors + 1\n",
    "            error_rate = 1.0 * errors / len(test_labels)\n",
    "            \n",
    "            self.test_x.append(self.test_x[-1]+1)\n",
    "            self.test_y.append(error_rate)\n",
    "            \n",
    "            print iterations,' iterations, test error rate = ',error_rate\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "                \n",
    "    def sigmoid(self, t):\n",
    "        \n",
    "        return 1.0/(1.0+np.exp(-t))\n",
    "\n",
    "    def derivative_sigmoid(self, t): \n",
    "        return t * (1.0 - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ffnet = FeedForwardNeuralNetwork2Hidden(784, 20, 10, 10)\n",
    "ffnet.train(training_samples,training_labels,test_samples,test_labels,0.1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,5)),plt.subplot(121),\n",
    "plt.plot(ffnet.training_x[1:],ffnet.training_y[1:],'b', label='training error')\n",
    "plt.plot(ffnet.test_x[1:],ffnet.test_y[1:],'r', label='test error')\n",
    "plt.title('Training and test error rate on network with 2 hidden layers')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('error rate')\n",
    "plt.legend(loc='upper center', shadow=True)\n",
    "plt.ylim(min(min(ffnet.training_y[1:]),min(ffnet.test_y[1:])),max(max(ffnet.training_y[1:]),max(ffnet.test_y[1:])))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (20000L, 32L, 32L, 3L)\n",
      "20000 train samples\n",
      "5000 test samples\n",
      "Not using data augmentation.\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      " 4000/20000 [=====>........................] - ETA: 142s - loss: 2.1550 - acc: 0.2040"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-021a6c9aa064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m               shuffle=True)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using real-time data augmentation.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ljt\\Anaconda2\\lib\\site-packages\\keras\\models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32mC:\\Users\\ljt\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ljt\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ljt\\Anaconda2\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ljt\\Anaconda2\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ljt\\Anaconda2\\lib\\site-packages\\theano\\gof\\op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "GPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatx=float32 python cifar10_cnn.py\n",
    "It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "data_augmentation = False\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train[:20000]\n",
    "y_train = y_train[:20000]\n",
    "x_test = x_test[:5000]\n",
    "y_test = y_test[:5000]\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# first convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# second convolutional layer\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# first max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# third convolutional layer\n",
    "#model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "# fourth convolutional layer\n",
    "#model.add(Conv2D(64, (3, 3)))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "# second max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# fully connected\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(512))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "#opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "opt = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    results = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
