{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train = []\n",
    "train_labels = []\n",
    "train_both = []\n",
    "file = open('iris/iris_train.data', 'r') \n",
    "for line in file: \n",
    "    l = line.rstrip('\\n').split(',')\n",
    "    \n",
    "    nums = [float(x) for x in l[:4]]\n",
    "    train_labels.append(l[-1])\n",
    "    nums.append(l[-1])\n",
    "    train_both.append(nums)\n",
    "    train.append(nums[0:-1])\n",
    "    \n",
    "    \n",
    "test = []\n",
    "test_labels = []\n",
    "test_both = []\n",
    "file = open('iris/iris_test.data', 'r') \n",
    "for line in file: \n",
    "    l = line.rstrip('\\n').split(',')\n",
    "    \n",
    "    nums = [float(x) for x in l[:4]]\n",
    "    test_labels.append(l[-1])\n",
    "    nums.append(l[-1])\n",
    "    test_both.append(nums)\n",
    "    test.append(nums[0:-1])\n",
    "\n",
    "test = np.array(test)\n",
    "test_labels = np.array(test_labels)\n",
    "test_both = np.array(test_both)\n",
    "#print train_both\n",
    "#print train\n",
    "train = np.array(train)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "train, train_labels = shuffle(train, train_labels, random_state=0)\n",
    "\n",
    "unique_labels = np.unique(train_labels)\n",
    "\n",
    "\n",
    "\n",
    "sepal_length_setosa = [x[0] for x in train_both if x[-1] == unique_labels[0]]\n",
    "sepal_width_setosa = [x[1] for x in train_both if x[-1] == unique_labels[0]]\n",
    "petal_length_setosa = [x[2] for x in train_both if x[-1] == unique_labels[0]]\n",
    "petal_width_setosa = [x[3] for x in train_both if x[-1] == unique_labels[0]]\n",
    "\n",
    "sepal_length_versicolor = [x[0] for x in train_both if x[-1] == unique_labels[1]]\n",
    "sepal_width_versicolor = [x[1] for x in train_both if x[-1] == unique_labels[1]]\n",
    "petal_length_versicolor = [x[2] for x in train_both if x[-1] == unique_labels[1]]\n",
    "petal_width_versicolor = [x[3] for x in train_both if x[-1] == unique_labels[1]]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10)),plt.subplot(231)\n",
    "plt.plot(sepal_length_setosa, sepal_width_setosa, 'g^',label='setosa')\n",
    "plt.plot(sepal_length_versicolor, sepal_width_versicolor, 'bo',label='versicolor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "\n",
    "\n",
    "plt.subplot(232),plt.plot(sepal_length_setosa, petal_length_setosa, 'g^',label='setosa')\n",
    "plt.plot(sepal_length_versicolor, petal_length_versicolor, 'bo',label='versicolor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal length')\n",
    "\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.plot(sepal_length_setosa, petal_width_setosa, 'g^',label='setosa')\n",
    "plt.plot(sepal_length_versicolor, petal_width_versicolor, 'bo',label='versicolor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('petal width')\n",
    "\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.plot(sepal_width_setosa, petal_length_setosa, 'g^',label='setosa')\n",
    "plt.plot(sepal_width_versicolor, petal_length_versicolor, 'bo',label='versicolor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('sepal width')\n",
    "plt.ylabel('petal length')\n",
    "\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.plot(sepal_width_setosa, petal_width_setosa, 'g^',label='setosa')\n",
    "plt.plot(sepal_width_versicolor, petal_width_versicolor, 'bo',label='versicolor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('sepal width')\n",
    "plt.ylabel('petal width')\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.plot(petal_length_setosa, petal_width_setosa, 'g^',label='setosa')\n",
    "plt.plot(petal_length_versicolor, petal_width_versicolor, 'bo',label='versicolor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('petal length')\n",
    "plt.ylabel('petal width')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def perceptronAccuracy(data, labels, weights):\n",
    "\n",
    "    missed = 0\n",
    "\n",
    "    testWithBias = np.append(np.ones((len(data),1)),data,axis=1)\n",
    "\n",
    "    for sample,label in zip(testWithBias,labels): \n",
    "        predicted = int(np.dot(sample,weights) >= 0)\n",
    "        \n",
    "    \n",
    "        if predicted != label: \n",
    "            missed = missed + 1\n",
    "\n",
    "    correct = len(labels) - missed \n",
    "    accuracy = 1.0 * correct / len(labels)\n",
    "    print \"Perceptron classifiction accuracy on test set = \",accuracy * 100,\"%\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trainPerceptron(learn_rate, data, labels): \n",
    "    weights = np.array([0.0 for x in range(data.shape[1]+1)])\n",
    "    errors = 1\n",
    "    iterations = 0\n",
    "    \n",
    "    trainWithBias = np.append(np.ones((len(data),1)),data,axis=1)\n",
    "    \n",
    "    while errors: \n",
    "        errors = 0\n",
    "        iterations = iterations + 1 \n",
    "\n",
    "        for sample,label in zip(trainWithBias,labels): \n",
    "\n",
    "            predicted = int(np.dot(sample,weights) >= 0)\n",
    "\n",
    "            error = label - predicted\n",
    "\n",
    "            if error != 0: \n",
    "                errors = errors + 1\n",
    "\n",
    "            weights = weights + (learn_rate * error * sample)\n",
    "            \n",
    "    print \"Number of iterations till convergence = \",iterations \n",
    "    return weights \n",
    "    \n",
    "    \n",
    "    \n",
    "learning_rate = 0.1\n",
    "\n",
    "print \"learning rate = \",learning_rate\n",
    "\n",
    "train_labels_perceptron = np.array([0 if x == unique_labels[0] else 1 for x in train_labels])\n",
    "test_labels_perceptron = np.array([0 if x == unique_labels[0] else 1 for x in test_labels])\n",
    "\n",
    "w = trainPerceptron(learning_rate, train, train_labels_perceptron)        \n",
    "perceptronAccuracy(test, test_labels_perceptron, w)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Z-score the training and test data \n",
    "\n",
    "trainZScored = np.copy(train)\n",
    "testZScored = np.copy(test)\n",
    "\n",
    "for i in range(train.shape[1]): \n",
    "    m = np.mean(train[:,i])\n",
    "    s = np.std(train[:,i])\n",
    "    \n",
    "    for x in range(len(trainZScored[:,i])):\n",
    "        \n",
    "        trainZScored[x,i] = (trainZScored[x,i] - m) / s\n",
    "\n",
    "    for x in range(len(testZScored[:,i])): \n",
    "        testZScored[x,i] = (testZScored[x,i] - m) / s\n",
    "        \n",
    "        \n",
    "        \n",
    "learning_rate = 0.1\n",
    "\n",
    "print \"learning rate = \",learning_rate\n",
    "wZScored = trainPerceptron(learning_rate, trainZScored, train_labels_perceptron)        \n",
    "perceptronAccuracy(testZScored, test_labels_perceptron, wZScored)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "training_samples = []\n",
    "training_labels = []\n",
    "for input,label in training_data: \n",
    "    training_samples.append(input)\n",
    "    training_labels.append(label)\n",
    "    \n",
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "for input,label in test_data: \n",
    "    test_samples.append(input)\n",
    "    test_labels.append(label)\n",
    "    \n",
    "training_samples = np.array(training_samples).reshape((50000,1,784))\n",
    "training_labels = np.array(training_labels).reshape((50000,10))\n",
    "test_samples = np.array(test_samples).reshape((10000,1,784))\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeedForwardNeuralNetwork: \n",
    "    def __init__(self, size_input, size_hidden, size_output):\n",
    "        \n",
    "        self.input_size = size_input\n",
    "        self.hidden_size = size_hidden\n",
    "        self.output_size = size_output\n",
    "        \n",
    "        self.hidden_weight = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.output_weight = np.random.randn(self.hidden_size, self.output_size)\n",
    "        \n",
    "        self.hidden_bias = np.random.randn(1,self.hidden_size) \n",
    "        self.output_bias = np.random.randn(1,self.output_size)\n",
    "        \n",
    "        self.output_velocity = np.zeros(self.output_weight.shape)\n",
    "        self.hidden_velocity = np.zeros(self.hidden_weight.shape)\n",
    "        \n",
    "        self.output_bias_velocity = np.zeros(self.output_bias.shape)\n",
    "        self.hidden_bias_velocity = np.zeros(self.hidden_bias.shape)\n",
    "        \n",
    "        self.training_x = [0]\n",
    "        self.training_y = [1]\n",
    "        \n",
    "        self.test_x = [0]\n",
    "        self.test_y = [1]\n",
    "        \n",
    "    def feed_forward(self, input): \n",
    "        \n",
    "        self.current_inputs = input\n",
    "        \n",
    "        # calculate hidden output -> output = (Inputs * weights) + biases\n",
    "        # apply activation function -> hidden activation = sigmoid(output)\n",
    "        \n",
    "        # (1,5)\n",
    "        self.hidden_activations = self.sigmoid(np.dot(input,self.hidden_weight) + self.hidden_bias)\n",
    "        \n",
    "        \n",
    "        # calculate output output -> output = (hidden activations * weights) + biases\n",
    "        # apply activation function -> output activation = sigmoid(output)\n",
    "        self.output_activations = self.sigmoid(np.dot(self.hidden_activations,self.output_weight) + self.output_bias)\n",
    "        \n",
    "        return self.output_activations \n",
    "    \n",
    "    def feed_forward_regularization(self, input, p): \n",
    "        \n",
    "        self.current_inputs = input\n",
    "        \n",
    "        # calculate hidden output -> output = (Inputs * weights) + biases\n",
    "        # apply activation function -> hidden activation = sigmoid(output)\n",
    "        \n",
    "        # (1,5)\n",
    "        self.hidden_activations = self.sigmoid(np.dot(input,self.hidden_weight) + self.hidden_bias)\n",
    "        \n",
    "        \n",
    "        drop_hidden = (np.random.rand(*self.hidden_activations.shape) < p ) \n",
    "        \n",
    "        self.hidden_activations = self.hidden_activations * drop_hidden\n",
    "        \n",
    "        \n",
    "        # calculate output output -> output = (hidden activations * weights) + biases\n",
    "        # apply activation function -> output activation = sigmoid(output)\n",
    "        self.output_activations = self.sigmoid(np.dot(self.hidden_activations,self.output_weight) + self.output_bias)\n",
    "        \n",
    "        return self.output_activations \n",
    "        \n",
    "    # alpha = Learning rate \n",
    "    def backpropagation_mom(self, output, alpha, mu):\n",
    "\n",
    "        target = output\n",
    "        \n",
    "        #(1,10)\n",
    "        output_delta = (self.output_activations - target) * self.derivative_sigmoid(self.output_activations)\n",
    "        \n",
    "        #(1,5)\n",
    "        hidden_delta = (np.dot(output_delta,self.output_weight.T)) * self.derivative_sigmoid(self.hidden_activations)\n",
    "        \n",
    "        #(1,784) \n",
    "        hidden_gradient = np.dot(self.current_inputs.T,hidden_delta)\n",
    "        \n",
    "        # (1,5)(1,10)\n",
    "        output_gradient = np.dot(self.hidden_activations.T, output_delta)\n",
    "        \n",
    "        output_bias_gradient = output_delta\n",
    "        hidden_bias_gradient = hidden_delta\n",
    "        \n",
    "        self.output_velocity = self.output_velocity * mu + (-1 * alpha * output_gradient)\n",
    "        self.output_weight = self.output_weight + self.output_velocity\n",
    "        \n",
    "        self.hidden_velocity = self.hidden_velocity * mu + (-1 * alpha * hidden_gradient)\n",
    "        self.hidden_weight = self.hidden_weight + self.hidden_velocity\n",
    "        \n",
    "        #self.output_weight = self.output_weight + (-1 * alpha * output_gradient)\n",
    "        #self.hidden_weight = self.hidden_weight + (-1 * alpha * hidden_gradient)\n",
    "        \n",
    "        self.output_bias_velocity = self.output_bias_velocity * mu + (-1 * alpha * output_bias_gradient)\n",
    "        self.output_bias = self.output_bias + self.output_bias_velocity\n",
    "        \n",
    "        self.hidden_bias_velocity = self.hidden_bias_velocity * mu + (-1 * alpha * hidden_bias_gradient)\n",
    "        self.hidden_bias = self.hidden_bias + self.hidden_bias_velocity\n",
    "        \n",
    "        #self.output_bias = self.output_bias + (-1 * alpha * output_bias_gradient)\n",
    "        #self.hidden_bias = self.hidden_bias + (-1 * alpha * hidden_bias_gradient)\n",
    "        \n",
    "    # alpha = Learning rate \n",
    "    def backpropagation(self, output, alpha):\n",
    "        target = output\n",
    "        \n",
    "        #(1,10)\n",
    "        output_delta = (self.output_activations - target) * self.derivative_sigmoid(self.output_activations)\n",
    "        \n",
    "        #(1,5)\n",
    "        hidden_delta = (np.dot(output_delta,self.output_weight.T)) * self.derivative_sigmoid(self.hidden_activations)\n",
    "        \n",
    "        #(1,784) \n",
    "        hidden_gradient = np.dot(self.current_inputs.T,hidden_delta)\n",
    "        \n",
    "        # (1,5)(1,10)\n",
    "        output_gradient = np.dot(self.hidden_activations.T, output_delta)\n",
    "        \n",
    "        output_bias_gradient = output_delta\n",
    "        hidden_bias_gradient = hidden_delta\n",
    "        \n",
    "        self.output_weight = self.output_weight + (-1 * alpha * output_gradient)\n",
    "        self.hidden_weight = self.hidden_weight + (-1 * alpha * hidden_gradient)\n",
    "        \n",
    "        self.output_bias = self.output_bias + (-1 * alpha * output_bias_gradient)\n",
    "        self.hidden_bias = self.hidden_bias + (-1 * alpha * hidden_bias_gradient)\n",
    "    \n",
    "    def predict(self,input, withRegMom, p): \n",
    "        \n",
    "        predictions = []\n",
    "        for i in input: \n",
    "            if withRegMom: \n",
    "                predictions.append(self.feed_forward_regularization(i,p))\n",
    "            else: \n",
    "                predictions.append(self.feed_forward(i))\n",
    "        return np.array(predictions)\n",
    "            \n",
    "    def train(self, training_samples, training_labels, test_samples, test_labels, alpha, epochs, withRegMom, p, mu):\n",
    "        \n",
    "        iterations = 0\n",
    "        for i in range(epochs): \n",
    "            \n",
    "            for input,label in zip(training_samples, training_labels):\n",
    "                if withRegMom: \n",
    "                    self.feed_forward_regularization(input,p)\n",
    "                    self.backpropagation_mom(label,alpha,mu)\n",
    "                else: \n",
    "                    self.feed_forward(input)\n",
    "                    self.backpropagation(label, alpha)\n",
    "                iterations = iterations + 1\n",
    "            \n",
    "        \n",
    "            #predictions = np.argmax(np.asarray(self.predict(training_samples)), axis=1)\n",
    "           \n",
    "            targets = np.argmax(np.asarray(training_labels), axis=1)\n",
    "            predictions = self.predict(training_samples, withRegMom, p)\n",
    "            \n",
    "            \n",
    "            errors = 0\n",
    "            for target,predicted in zip(targets,predictions): \n",
    "                predicted = np.argmax(np.asarray(predicted), axis=1)\n",
    "                if target != predicted: \n",
    "                    errors = errors + 1\n",
    "            error_rate = 1.0 * errors / len(training_labels)\n",
    "            \n",
    "            print iterations,' iterations, training error rate = ',error_rate\n",
    "            \n",
    "            self.training_x.append(self.training_x[-1]+1)\n",
    "            self.training_y.append(error_rate)\n",
    "            \n",
    "            \n",
    "            \n",
    "            targets = test_labels\n",
    "            predictions = self.predict(test_samples, withRegMom, p)\n",
    "            \n",
    "            \n",
    "            errors = 0\n",
    "            for target,predicted in zip(targets,predictions): \n",
    "                predicted = np.argmax(np.asarray(predicted), axis=1)\n",
    "                if target != predicted: \n",
    "                    errors = errors + 1\n",
    "            error_rate = 1.0 * errors / len(test_labels)\n",
    "            \n",
    "            self.test_x.append(self.test_x[-1]+1)\n",
    "            self.test_y.append(error_rate)\n",
    "            \n",
    "            print iterations,' iterations, test error rate = ',error_rate\n",
    "            \n",
    "    \n",
    "                \n",
    "    def sigmoid(self, t):\n",
    "        \n",
    "        return 1.0/(1.0+np.exp(-t))\n",
    "\n",
    "    def derivative_sigmoid(self, t): \n",
    "        return t * (1.0 - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ffnet = FeedForwardNeuralNetwork(784, 20, 10)\n",
    "ffnet.train(training_samples,training_labels,test_samples,test_labels,0.1,60, False, 0,0)\n",
    "#ffnetRegMom.train(training_samples,training_labels,test_samples,test_labels,0.1,100, True, 0.95,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)),plt.subplot(121),\n",
    "plt.plot(ffnet.training_x[1:],ffnet.training_y[1:],'b', label='training error')\n",
    "plt.plot(ffnet.test_x[1:],ffnet.test_y[1:],'r', label='test error')\n",
    "plt.title('Training and test error rate on feedforward network')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('error rate')\n",
    "plt.legend(loc='upper center', shadow=True)\n",
    "plt.ylim(min(min(ffnet.training_y[1:]),min(ffnet.test_y[1:])),max(max(ffnet.training_y[1:]),max(ffnet.test_y[1:])))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ffnetRegMom = FeedForwardNeuralNetwork(784, 20, 10)\n",
    "#ffnet.train(training_samples,training_labels,test_samples,test_labels,0.1,60, False, 0,0)\n",
    "ffnetRegMom.train(training_samples,training_labels,test_samples,test_labels,0.1,60, True, 0.95,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)),plt.subplot(121),\n",
    "plt.plot(ffnetRegMom.training_x[1:],ffnetRegMom.training_y[1:],'b', label='training error')\n",
    "plt.plot(ffnetRegMom.test_x[1:],ffnetRegMom.test_y[1:],'r', label='test error')\n",
    "plt.title('Training and test error rate on network with regularization and momentum')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('error rate')\n",
    "plt.legend(loc='upper center', shadow=True)\n",
    "plt.ylim(min(min(ffnetRegMom.training_y[1:]),min(ffnetRegMom.test_y[1:])),max(max(ffnetRegMom.training_y[1:]),max(ffnetRegMom.test_y[1:])))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork2Hidden: \n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2, size_output):\n",
    "        \n",
    "        self.input_size = size_input\n",
    "        self.hidden1_size = size_hidden1\n",
    "        self.hidden2_size = size_hidden2\n",
    "        self.output_size = size_output\n",
    "        \n",
    "        self.hidden1_weight = np.random.randn(self.input_size, self.hidden1_size)\n",
    "        self.hidden2_weight = np.random.randn(self.hidden1_size, self.hidden2_size)\n",
    "        self.output_weight = np.random.randn(self.hidden2_size, self.output_size)\n",
    "        \n",
    "        self.hidden1_bias = np.random.randn(1,self.hidden1_size) \n",
    "        self.hidden2_bias = np.random.randn(1,self.hidden2_size) \n",
    "        self.output_bias = np.random.randn(1,self.output_size)\n",
    "\n",
    "        self.training_x = [0]\n",
    "        self.training_y = [1]\n",
    "        \n",
    "        self.test_x = [0]\n",
    "        self.test_y = [1]\n",
    "\n",
    "        \n",
    "       \n",
    "    def feed_forward(self, input): \n",
    "        \n",
    "        self.current_inputs = input\n",
    "        \n",
    "        # calculate hidden output -> output = (Inputs * weights) + biases\n",
    "        # apply activation function -> hidden activation = sigmoid(output)\n",
    "        \n",
    "        # (1,5)\n",
    "        self.hidden1_activations = self.sigmoid(np.dot(input,self.hidden1_weight) + self.hidden1_bias)\n",
    "        \n",
    "        \n",
    "        self.hidden2_activations = self.sigmoid(np.dot(self.hidden1_activations,self.hidden2_weight) + self.hidden2_bias)\n",
    "        \n",
    "        \n",
    "        # calculate output output -> output = (hidden activations * weights) + biases\n",
    "        # apply activation function -> output activation = sigmoid(output)\n",
    "        self.output_activations = self.sigmoid(np.dot(self.hidden2_activations,self.output_weight) + self.output_bias)\n",
    "        \n",
    "        return self.output_activations \n",
    "    \n",
    "   \n",
    "        \n",
    "    # alpha = Learning rate \n",
    "    def backpropagation(self, output, alpha):\n",
    "        target = output\n",
    "        \n",
    "        #(1,10)\n",
    "        output_delta = (self.output_activations - target) * self.derivative_sigmoid(self.output_activations)\n",
    "        \n",
    "        #(1,5)\n",
    "        hidden2_delta = (np.dot(output_delta,self.output_weight.T)) * self.derivative_sigmoid(self.hidden2_activations)\n",
    "        \n",
    "        hidden1_delta = (np.dot(hidden2_delta,self.hidden2_weight.T)) * self.derivative_sigmoid(self.hidden1_activations)\n",
    "        \n",
    "        # (1,5)(1,10)\n",
    "        output_gradient = np.dot(self.hidden2_activations.T, output_delta)\n",
    "        \n",
    "        #(1,784) \n",
    "        hidden2_gradient = np.dot(self.hidden1_activations.T,hidden2_delta)\n",
    "        \n",
    "        hidden1_gradient = np.dot(self.current_inputs.T,hidden1_delta)\n",
    "        \n",
    "       \n",
    "        \n",
    "        output_bias_gradient = output_delta\n",
    "        hidden2_bias_gradient = hidden2_delta\n",
    "        hidden1_bias_gradient = hidden1_delta\n",
    "        \n",
    "        self.output_weight = self.output_weight + (-1 * alpha * output_gradient)\n",
    "        self.hidden2_weight = self.hidden2_weight + (-1 * alpha * hidden2_gradient)\n",
    "        self.hidden1_weight = self.hidden1_weight + (-1 * alpha * hidden1_gradient)\n",
    "        \n",
    "        self.output_bias = self.output_bias + (-1 * alpha * output_bias_gradient)\n",
    "        self.hidden2_bias = self.hidden2_bias + (-1 * alpha * hidden2_bias_gradient)\n",
    "        self.hidden1_bias = self.hidden1_bias + (-1 * alpha * hidden1_bias_gradient)\n",
    "    \n",
    "    def predict(self,input): \n",
    "        \n",
    "        predictions = []\n",
    "        for i in input: \n",
    "            predictions.append(self.feed_forward(i))\n",
    "        return np.array(predictions)\n",
    "            \n",
    "    def train(self, training_samples, training_labels, test_samples, test_labels, alpha, epochs):\n",
    "        \n",
    "        iterations = 0\n",
    "        for i in range(epochs): \n",
    "            \n",
    "            for input,label in zip(training_samples, training_labels):\n",
    "                self.feed_forward(input)\n",
    "                self.backpropagation(label, alpha)\n",
    "                iterations = iterations + 1\n",
    "            \n",
    "        \n",
    "            #predictions = np.argmax(np.asarray(self.predict(training_samples)), axis=1)\n",
    "           \n",
    "            targets = np.argmax(np.asarray(training_labels), axis=1)\n",
    "            predictions = self.predict(training_samples)\n",
    "            \n",
    "            \n",
    "            errors = 0\n",
    "            for target,predicted in zip(targets,predictions): \n",
    "                predicted = np.argmax(np.asarray(predicted), axis=1)\n",
    "                if target != predicted: \n",
    "                    errors = errors + 1\n",
    "            error_rate = 1.0 * errors / len(training_labels)\n",
    "            self.training_x.append(self.training_x[-1]+1)\n",
    "            self.training_y.append(error_rate)\n",
    "            \n",
    "            \n",
    "            print iterations,' iterations, training error rate = ',error_rate\n",
    "            \n",
    "            \n",
    "            targets = test_labels\n",
    "            predictions = self.predict(test_samples)\n",
    "            \n",
    "            \n",
    "            errors = 0\n",
    "            for target,predicted in zip(targets,predictions): \n",
    "                predicted = np.argmax(np.asarray(predicted), axis=1)\n",
    "                if target != predicted: \n",
    "                    errors = errors + 1\n",
    "            error_rate = 1.0 * errors / len(test_labels)\n",
    "            \n",
    "            self.test_x.append(self.test_x[-1]+1)\n",
    "            self.test_y.append(error_rate)\n",
    "            \n",
    "            print iterations,' iterations, test error rate = ',error_rate\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "                \n",
    "    def sigmoid(self, t):\n",
    "        \n",
    "        return 1.0/(1.0+np.exp(-t))\n",
    "\n",
    "    def derivative_sigmoid(self, t): \n",
    "        return t * (1.0 - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ffnet = FeedForwardNeuralNetwork2Hidden(784, 20, 20, 10)\n",
    "ffnet.train(training_samples,training_labels,test_samples,test_labels,0.1,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,5)),plt.subplot(121),\n",
    "plt.plot(ffnet.training_x[1:],ffnet.training_y[1:],'b', label='training error')\n",
    "plt.plot(ffnet.test_x[1:],ffnet.test_y[1:],'r', label='test error')\n",
    "plt.title('Training and test error rate on network with 2 hidden layers')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('error rate')\n",
    "plt.legend(loc='upper center', shadow=True)\n",
    "plt.ylim(min(min(ffnet.training_y[1:]),min(ffnet.test_y[1:])),max(max(ffnet.training_y[1:]),max(ffnet.test_y[1:])))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# modified from https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# first convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# second convolutional layer\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# first max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# third convolutional layer\n",
    "#model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "# fourth convolutional layer\n",
    "#model.add(Conv2D(64, (3, 3)))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "# second max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# fully connected\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "#opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "#opt = keras.optimizers.SGD(lr=0.008, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "lrate = 0.01\n",
    "epochs = 30\n",
    "decay = lrate/epochs\n",
    "sgd = keras.optimizers.SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True)\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)),plt.subplot(121),\n",
    "x1 = range(epochs)\n",
    "train_loss = results.history['loss']\n",
    "x2 = range(epochs)\n",
    "test_loss = results.history['val_loss']\n",
    "plt.plot(x1,train_loss,'b', label='training loss')\n",
    "plt.plot(x2,test_loss,'r', label='test loss')\n",
    "plt.title('Training and test loss for convolutional neural network with SGD')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper center', shadow=True)\n",
    "plt.ylim(min(min(train_loss),min(test_loss)),max(max(train_loss),max(test_loss)))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## CNN with bath normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modified from https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# first convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# second convolutional layer\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# first max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# third convolutional layer\n",
    "#model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "# fourth convolutional layer\n",
    "#model.add(Conv2D(64, (3, 3)))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "# second max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# fully connected\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "#opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "#opt = keras.optimizers.SGD(lr=0.008, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "lrate = 0.01\n",
    "epochs = 30\n",
    "decay = lrate/epochs\n",
    "sgd = keras.optimizers.SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results2 = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True)\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)),plt.subplot(121),\n",
    "x1 = range(epochs)\n",
    "train_loss = results2.history['loss']\n",
    "x2 = range(epochs)\n",
    "test_loss = results2.history['val_loss']\n",
    "plt.plot(x1,train_loss,'b', label='training loss')\n",
    "plt.plot(x2,test_loss,'r', label='test loss')\n",
    "plt.title('Training and test loss for convolutional neural network with batch normalization')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper center', shadow=True)\n",
    "plt.ylim(min(min(train_loss),min(test_loss)),max(max(train_loss),max(test_loss)))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with global average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modified from https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D, Embedding, LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# first convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# second convolutional layer\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# first max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# third convolutional layer\n",
    "#model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "# fourth convolutional layer\n",
    "#model.add(Conv2D(64, (3, 3)))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "# second max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# fully connected\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(512))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(num_classes))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "#opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "#opt = keras.optimizers.SGD(lr=0.008, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "lrate = 0.01\n",
    "epochs = 50\n",
    "decay = lrate/epochs\n",
    "sgd = keras.optimizers.SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "results2 = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True)\n",
    "\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)),plt.subplot(121),\n",
    "x1 = range(epochs)\n",
    "train_loss = results2.history['loss']\n",
    "x2 = range(epochs)\n",
    "test_loss = results2.history['val_loss']\n",
    "plt.plot(x1,train_loss,'b', label='training loss')\n",
    "plt.plot(x2,test_loss,'r', label='test loss')\n",
    "plt.title('Training and test loss for CNN with Global Average Pooling ')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper center', shadow=True)\n",
    "plt.ylim(min(min(train_loss),min(test_loss)),max(max(train_loss),max(test_loss)))\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
